[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Gregory Ehlinger",
    "section": "",
    "text": "Download Resume"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gregory Ehlinger",
    "section": "",
    "text": "Gregory Ehlinger is currently pursuing his Masters of Science in Advanced Data Analytics at the University of North Texas. He completed his Bachelors of Science in Industrial Distribution from Texas A&M University. For the past 6 years, he has been working as an Operations Associate and Inside Sales for a plumbing wholesaler that serves the North Texas Region.\nCompetent in handling cost analysis, quality documentation, process improvement & implementation, procurement, and direct interaction with customers & vendors.\nProficient in Tableau, Eclipse, RStudio, Python, Eclipse, and Microsoft Office.\n \n  \n   \n  \n    \n     Email\n  \n  \n    \n     GitHub\n  \n  \n    \n     Resume"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\nGregory Ehlinger\nLearn more about Quarto here."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "The Effect of the Pitch Clock on MLB Pitchers\n\n\n\n\n\n\n\nPython\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2023\n\n\nDiana Bergeman, Eric Droegemeier, Gregory Ehlinger, Triniti Lemmons, Mauricio Sanchez\n\n\n\n\n\n\n  \n\n\n\n\nAnalysis and Prediction on Donations\n\n\n\n\n\n\n\nSAS Enterprise Miner\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2023\n\n\nEric Droegemeier, Gregory Ehlinger, Veda Jenetty Immaraj\n\n\n\n\n\n\n  \n\n\n\n\nTexas Census Data\n\n\n\n\n\n\n\nGoogle Cloud Services\n\n\nTableau\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 23, 2023\n\n\nEric Droegemeier, Gregory Ehlinger, Tserendulam Ichinnorov, Paloma Leonato, Brad Reese\n\n\n\n\n\n\n  \n\n\n\n\nAnalysis and Prediction on Cryptocurrencies\n\n\n\n\n\n\n\nExcel\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 13, 2022\n\n\nEric Droegemeier, Gregory Ehlinger, Tejaskumar Patel, Brad Reese\n\n\n\n\n\n\n  \n\n\n\n\nLeBron James Career Breakdown\n\n\n\n\n\n\n\nTableau\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2022\n\n\nGregory Ehlinger, Veda Jenetty Immaraj\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Intro to Linear Regression in RStudio\n\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2023\n\n\nGregory Ehlinger\n\n\n\n\n\n\n  \n\n\n\n\nIntro to EDA in RStudio\n\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2023\n\n\nGregory Ehlinger\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html#lab-reports",
    "href": "projects.html#lab-reports",
    "title": "Projects",
    "section": "",
    "text": "Learn more about Quarto here."
  },
  {
    "objectID": "projects.html#meeting-notes",
    "href": "projects.html#meeting-notes",
    "title": "Projects",
    "section": "Meeting Notes",
    "text": "Meeting Notes\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\nLearn more about Quarto here."
  },
  {
    "objectID": "posts/post with code/index.html",
    "href": "posts/post with code/index.html",
    "title": "LeBron James Career Breakdown",
    "section": "",
    "text": "Analyzes the statistics from LeBron James’s first 18 seasons in the NBA.\nLeBron James Career Breakdown Tableau Dashboard Download\nLeBron James Career Breakdown Dataset Download\n\n\n\nFor this Project, Jenetty Immaraj and I analyzed the first 18 years of LeBron James’s NBA Career. We created a Tableau dashboard to visualize various statistics for each year. Visualizations include profile photo and jerseys, biometrics, statistical averages, shot selection, and geolocation for recorded statistics."
  },
  {
    "objectID": "projects.html#projects",
    "href": "projects.html#projects",
    "title": "Projects",
    "section": "",
    "text": "quarto::doc_list( dir = “path/to/your/documents”, exclude = c(“*.ipynb”, “post”) )"
  },
  {
    "objectID": "posts/census data/index.html",
    "href": "posts/census data/index.html",
    "title": "Texas Census Data",
    "section": "",
    "text": "Analyzes 2021 US Census data to compare income and employment of Foreign-born adults to US-born adults who all live in the state of Texas.\nCensus Data Presentation Download\nCensus Data Code Download\n\n\nFor this Project, Eric Droegemeier, Tserendulam Ichinnorov, Paloma Leonato, Brad Reese and I analyzed the census data for the state of Texas in 2021. We specifcally wanted to compare the income and employment of workers who were foreign-born to those of workers born in the United States. For this project, we used Google Cloud Services, Tableau, and Python."
  },
  {
    "objectID": "posts/census data/adta5240_linear_regression.html",
    "href": "posts/census data/adta5240_linear_regression.html",
    "title": "\n 5240 Group 1 - March the 26th, 2023\n\n",
    "section": "",
    "text": "##\n\nLinear Regression: TX Income based on demographics"
  },
  {
    "objectID": "posts/census data/adta5240_linear_regression.html#section",
    "href": "posts/census data/adta5240_linear_regression.html#section",
    "title": "\n 5240 Group 1 - March the 26th, 2023\n\n",
    "section": "",
    "text": "##\n\nLinear Regression: TX Income based on demographics"
  },
  {
    "objectID": "posts/census data/adta5240_linear_regression.html#ml-aproach",
    "href": "posts/census data/adta5240_linear_regression.html#ml-aproach",
    "title": "\n 5240 Group 1 - March the 26th, 2023\n\n",
    "section": "ML Aproach",
    "text": "ML Aproach\n\n\n\naproach.PNG\n\n\n\nImport Libraries\n\nimport pandas and numpy libraries\nimport scatter_matrix from pandas.plotting\nimport LinearRegression from sklearn.linear_model\nimport train_test_split, KFold, and cross_val_score from sklearn.model_selection\nimport matplotlib\nimport seaborn\n\n\nimport pandas as pd\nimport numpy as np\n\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nimport matplotlib as mp\nimport matplotlib.pyplot as plt\n\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sns"
  },
  {
    "objectID": "posts/census data/adta5240_linear_regression.html#load-data-set",
    "href": "posts/census data/adta5240_linear_regression.html#load-data-set",
    "title": "\n 5240 Group 1 - March the 26th, 2023\n\n",
    "section": " LOAD DATA SET ",
    "text": "LOAD DATA SET"
  },
  {
    "objectID": "posts/census data/adta5240_linear_regression.html#show-top-2-and-bottom-5-rows",
    "href": "posts/census data/adta5240_linear_regression.html#show-top-2-and-bottom-5-rows",
    "title": "\n 5240 Group 1 - March the 26th, 2023\n\n",
    "section": "2.1 Show top 2 and Bottom 5 rows",
    "text": "2.1 Show top 2 and Bottom 5 rows\n\ndf.head(2)\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2021GQ0000009\n7\n1\n4615\n3\n48\n1029928\n31\n21\n...\n33\n0\n30\n0\n30\n0\n1\n1\n0\n29\n\n\n1\nP\n2021GQ0000014\n7\n1\n2508\n3\n48\n1029928\n25\n19\n...\n51\n0\n25\n24\n25\n49\n0\n25\n24\n0\n\n\n\n\n2 rows × 287 columns\n\n\n\n\ndf.tail(5)\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n261441\nP\n2021HU1415640\n7\n1\n2322\n3\n48\n1029928\n46\n61\n...\n14\n45\n41\n81\n14\n50\n45\n15\n45\n71\n\n\n261442\nP\n2021HU1415640\n7\n2\n2322\n3\n48\n1029928\n78\n61\n...\n26\n80\n65\n138\n23\n77\n81\n26\n83\n130\n\n\n261443\nP\n2021HU1415668\n7\n1\n4629\n3\n48\n1029928\n248\n44\n...\n245\n242\n249\n429\n76\n406\n225\n71\n73\n429\n\n\n261444\nP\n2021HU1415668\n7\n2\n4629\n3\n48\n1029928\n288\n53\n...\n278\n271\n269\n482\n93\n526\n283\n81\n87\n506\n\n\n261445\nP\n2021HU1415668\n7\n3\n4629\n3\n48\n1029928\n303\n15\n...\n293\n302\n274\n543\n89\n505\n288\n85\n93\n500\n\n\n\n\n5 rows × 287 columns"
  },
  {
    "objectID": "posts/census data/adta5240_linear_regression.html#filter-columns",
    "href": "posts/census data/adta5240_linear_regression.html#filter-columns",
    "title": "\n 5240 Group 1 - March the 26th, 2023\n\n",
    "section": "3. Filter columns",
    "text": "3. Filter columns\n\ndf.columns\n\nIndex(['RT', 'SERIALNO', 'DIVISION', 'SPORDER', 'PUMA', 'REGION', 'ST',\n       'ADJINC', 'PWGTP', 'AGEP',\n       ...\n       'PWGTP71', 'PWGTP72', 'PWGTP73', 'PWGTP74', 'PWGTP75', 'PWGTP76',\n       'PWGTP77', 'PWGTP78', 'PWGTP79', 'PWGTP80'],\n      dtype='object', length=287)\n\n\n\nDescription of selected data fields\n\nST: State Code based on 2010 Census definitions\nAGEP: Age\nCOW: Class of worker\nENG: Ability to speak English\nSCHL: Educational attainment\nSEX: Sex\nWAGP: Wages or salary income past 12 months (use ADJINC to adjust WAGP to constant dollars)\nWKHP: Usual hours worked per week past 12 months\nJWAP: Time of arrival at work - hour and minute\nJWDP: Time of departure for work - hour and minute\nMIGSP: Migration recode - State or foreign country code\nNAICSP: North American Industry Classification System (NAICS) recode for 2018 and later based on 2017 NAICS codes\nNOP: Nativity of parent\nOCCP: Occupation recode for 2018 and later based on 2018 OCC codes\nPERNP: Total person’s earnings (use ADJINC to adjust to constant dollars)\nPINCP: Total person’s income (signed, use ADJINC to adjust to constant dollars)\nPOBP: Place of birth (Recode)\nSCIENGP: Field of Degree Science and Engineering Flag - NSF Definition\nSCIENGRLP: Field of Degree Science and Engineering Related Flag - NSF Definition\nSOCP: Standard Occupational Classification (SOC) codes for 2018 and later based on 2018 SOC codes\nWAOB: World area of birth\nFAGEP: Age allocation flag\nFSEMP: Self employment Flag\nPWGTP: Person’s weight\nRACASN: Asian recode (Asian alone or in combination with one or more other races)\nRACBLK: Black or African American recode (Black alone or in combination with one or more other races)\nRACWHT: White recode (White alone or in combination with one or more other races)\n\n\ndf2 = df[{'MIGSP','NAICSP','NOP','PERNP','PINCP','SCIENGP','SCIENGRLP','SOCP','WAOB','AGEP','SEX',\n          'POBP','COW','RT','ENG','SCHL','WAGP','WKHP','JWAP','JWDP','OCCP','POBP','ST','NATIVITY','FSEMP','PWGTP',\n          'RACASN','RACBLK','RACWHT'}]\n\nC:\\Users\\palom\\AppData\\Local\\Temp\\ipykernel_13200\\3040542100.py:1: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n  df2 = df[{'MIGSP','NAICSP','NOP','PERNP','PINCP','SCIENGP','SCIENGRLP','SOCP','WAOB','AGEP','SEX',\n\n\n\ndf2.head(10)\n\n\n\n\n\n\n\n\nST\nRACASN\nPINCP\nJWDP\nPERNP\nWAOB\nSEX\nNATIVITY\nRACWHT\nRACBLK\n...\nWAGP\nRT\nPWGTP\nCOW\nPOBP\nJWAP\nSOCP\nNOP\nOCCP\nFSEMP\n\n\n\n\n0\n48\n0\n7000.0\n118.0\n7000.0\n1\n1\n1\n1\n0\n...\n7000.0\nP\n31\n1.0\n48\n199.0\n353031\nNaN\n4110.0\n0\n\n\n1\n48\n0\n0.0\nNaN\n0.0\n1\n1\n1\n0\n1\n...\n0.0\nP\n25\nNaN\n36\nNaN\nNaN\nNaN\nNaN\n0\n\n\n2\n48\n0\n0.0\nNaN\n0.0\n1\n1\n1\n1\n0\n...\n0.0\nP\n19\n1.0\n48\nNaN\n472211\nNaN\n6520.0\n0\n\n\n3\n48\n0\n13500.0\nNaN\n0.0\n1\n1\n1\n0\n1\n...\n0.0\nP\n12\nNaN\n48\nNaN\nNaN\nNaN\nNaN\n0\n\n\n4\n48\n0\n10800.0\nNaN\n0.0\n1\n1\n1\n1\n0\n...\n0.0\nP\n16\nNaN\n48\nNaN\nNaN\nNaN\nNaN\n0\n\n\n5\n48\n0\n0.0\nNaN\n0.0\n5\n1\n2\n1\n0\n...\n0.0\nP\n56\nNaN\n110\nNaN\nNaN\nNaN\nNaN\n0\n\n\n6\n48\n0\n0.0\nNaN\n0.0\n1\n1\n1\n0\n0\n...\n0.0\nP\n60\nNaN\n48\nNaN\nNaN\nNaN\nNaN\n0\n\n\n7\n48\n0\n0.0\nNaN\n0.0\n1\n1\n1\n1\n0\n...\n0.0\nP\n25\nNaN\n6\nNaN\nNaN\nNaN\nNaN\n0\n\n\n8\n48\n0\n0.0\nNaN\n0.0\n1\n1\n1\n1\n0\n...\n0.0\nP\n8\nNaN\n48\nNaN\nNaN\nNaN\nNaN\n1\n\n\n9\n48\n0\n13000.0\n100.0\n13000.0\n1\n1\n1\n1\n0\n...\n13000.0\nP\n16\n1.0\n48\n162.0\n537065\nNaN\n9645.0\n0\n\n\n\n\n10 rows × 28 columns\n\n\n\n\nprint(df.shape)\n\n(261446, 287)\n\n\n\ndf2.dtypes\n\nST             int64\nRACASN         int64\nPINCP        float64\nJWDP         float64\nPERNP        float64\nWAOB           int64\nSEX            int64\nNATIVITY       int64\nRACWHT         int64\nRACBLK         int64\nSCIENGP      float64\nSCHL         float64\nMIGSP        float64\nNAICSP        object\nAGEP           int64\nSCIENGRLP    float64\nENG          float64\nWKHP         float64\nWAGP         float64\nRT            object\nPWGTP          int64\nCOW          float64\nPOBP           int64\nJWAP         float64\nSOCP          object\nNOP          float64\nOCCP         float64\nFSEMP          int64\ndtype: object"
  },
  {
    "objectID": "posts/census data/adta5240_linear_regression.html#clean-the-data",
    "href": "posts/census data/adta5240_linear_regression.html#clean-the-data",
    "title": "\n 5240 Group 1 - March the 26th, 2023\n\n",
    "section": "1. Clean the data",
    "text": "1. Clean the data\n\nFind and Mark Missing Values\n\n\ndf2.isnull().sum()\n\nST                0\nRACASN            0\nPINCP         45410\nJWDP         166266\nPERNP         48880\nWAOB              0\nSEX               0\nNATIVITY          0\nRACWHT            0\nRACBLK            0\nSCIENGP      197341\nSCHL           7952\nMIGSP        226887\nNAICSP       109139\nAGEP              0\nSCIENGRLP    197341\nENG          186811\nWKHP         131782\nWAGP          45410\nRT                0\nPWGTP             0\nCOW          109139\nPOBP              0\nJWAP         166266\nSOCP         109139\nNOP          208561\nOCCP         109139\nFSEMP             0\ndtype: int64\n\n\n\ndf3 = df2[df2['COW'].notna()]\n\n\ndf4 = df3[df3['WKHP'].notna()]\n\n\ndf4['SCIENGP'] = df4['SCIENGP'].fillna(0)\n\nC:\\Users\\palom\\AppData\\Local\\Temp\\ipykernel_13200\\2680412488.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df4['SCIENGP'] = df4['SCIENGP'].fillna(0)\n\n\n\nprint(df4.shape)\n\n(129664, 28)\n\n\n\ndf4.isnull().sum()\n\nST                0\nRACASN            0\nPINCP             0\nJWDP          34484\nPERNP             0\nWAOB              0\nSEX               0\nNATIVITY          0\nRACWHT            0\nRACBLK            0\nSCIENGP           0\nSCHL              0\nMIGSP        110606\nNAICSP            0\nAGEP              0\nSCIENGRLP     82852\nENG           88670\nWKHP              0\nWAGP              0\nRT                0\nPWGTP             0\nCOW               0\nPOBP              0\nJWAP          34484\nSOCP              0\nNOP          127911\nOCCP              0\nFSEMP             0\ndtype: int64\n\n\n\ndf5 = df4[{'NAICSP','PERNP','PINCP','SOCP','WAOB','AGEP','SEX','SCIENGP',\n          'POBP','COW','RT','SCHL','WAGP','WKHP','OCCP','POBP','FSEMP','RACASN','RACBLK','RACWHT','PWGTP'}]\n\nC:\\Users\\palom\\AppData\\Local\\Temp\\ipykernel_13200\\3447267456.py:1: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n  df5 = df4[{'NAICSP','PERNP','PINCP','SOCP','WAOB','AGEP','SEX','SCIENGP',\n\n\n\ndf5.isnull().sum()\n\nRACASN     0\nPINCP      0\nPERNP      0\nWAOB       0\nSEX        0\nRACWHT     0\nRACBLK     0\nSCIENGP    0\nSCHL       0\nNAICSP     0\nAGEP       0\nWKHP       0\nWAGP       0\nRT         0\nPWGTP      0\nCOW        0\nPOBP       0\nSOCP       0\nOCCP       0\nFSEMP      0\ndtype: int64\n\n\n\nprint(df5.shape)\n\n(129664, 20)"
  },
  {
    "objectID": "posts/census data/adta5240_linear_regression.html#performing-the-exploratory-data-analysis-eda",
    "href": "posts/census data/adta5240_linear_regression.html#performing-the-exploratory-data-analysis-eda",
    "title": "\n 5240 Group 1 - March the 26th, 2023\n\n",
    "section": "2. Performing the Exploratory Data Analysis (EDA)",
    "text": "2. Performing the Exploratory Data Analysis (EDA)\n\nPrint a count of the number of rows (observations) and columns (variables)\nPrint the data types of all variables\nPrint a summary statistics of the data\n\n\ndf5.describe()\n\n\n\n\n\n\n\n\nPOBP\nFSEMP\nRACBLK\nOCCP\nPINCP\nAGEP\nRACASN\nWAGP\nSEX\nSCIENGP\nCOW\nPERNP\nWKHP\nSCHL\nRACWHT\nPWGTP\nWAOB\n\n\n\n\ncount\n129664.000000\n129664.000000\n129664.000000\n129664.000000\n1.296640e+05\n129664.000000\n129664.000000\n129664.000000\n129664.000000\n129664.000000\n129664.000000\n129664.000000\n129664.000000\n129664.000000\n129664.000000\n129664.000000\n129664.000000\n\n\nmean\n88.852480\n0.158895\n0.099688\n4103.373118\n6.075186e+04\n42.917101\n0.069402\n52085.459650\n1.465627\n0.584511\n2.102635\n55968.915836\n39.121815\n18.277055\n0.744586\n117.242357\n1.542788\n\n\nstd\n102.200954\n0.365580\n0.299585\n2713.268667\n7.708049e+04\n15.277441\n0.254138\n67908.465601\n0.498819\n0.830562\n1.852596\n71724.136602\n13.094686\n3.875669\n0.436095\n108.011057\n1.150909\n\n\nmin\n1.000000\n0.000000\n0.000000\n10.000000\n-8.400000e+03\n16.000000\n0.000000\n0.000000\n1.000000\n0.000000\n1.000000\n-8400.000000\n1.000000\n1.000000\n0.000000\n1.000000\n1.000000\n\n\n25%\n47.000000\n0.000000\n0.000000\n2025.000000\n2.000000e+04\n30.000000\n0.000000\n13000.000000\n1.000000\n0.000000\n1.000000\n17000.000000\n36.000000\n16.000000\n0.000000\n51.000000\n1.000000\n\n\n50%\n48.000000\n0.000000\n0.000000\n4110.000000\n4.000000e+04\n42.000000\n0.000000\n35000.000000\n1.000000\n0.000000\n1.000000\n37500.000000\n40.000000\n19.000000\n1.000000\n84.000000\n1.000000\n\n\n75%\n48.000000\n0.000000\n0.000000\n5740.000000\n7.200000e+04\n55.000000\n0.000000\n65000.000000\n2.000000\n1.000000\n3.000000\n69000.000000\n43.000000\n21.000000\n1.000000\n145.000000\n1.000000\n\n\nmax\n554.000000\n1.000000\n1.000000\n9830.000000\n1.157000e+06\n92.000000\n1.000000\n489000.000000\n2.000000\n2.000000\n8.000000\n844000.000000\n99.000000\n24.000000\n1.000000\n2471.000000\n8.000000\n\n\n\n\n\n\n\n\ndf5.describe(include='object')\n\n\n\n\n\n\n\n\nNAICSP\nRT\nSOCP\n\n\n\n\ncount\n129664\n129664\n129664\n\n\nunique\n269\n1\n529\n\n\ntop\n23\nP\n1191XX\n\n\nfreq\n10174\n129664\n3697\n\n\n\n\n\n\n\n\ndf5['COW'].value_counts()\n\n1.0    86128\n3.0    10502\n6.0     9707\n2.0     8407\n4.0     5383\n7.0     4742\n5.0     4380\n8.0      415\nName: COW, dtype: int64\n\n\n\ndf5['WKHP'].value_counts()\n\n40.0    61789\n50.0    10865\n45.0     6707\n30.0     5796\n60.0     5507\n        ...  \n71.0        4\n89.0        3\n73.0        3\n93.0        1\n81.0        1\nName: WKHP, Length: 97, dtype: int64\n\n\n\ndf5['SCHL'].value_counts()\n\n21.0    29627\n16.0    25361\n19.0    18732\n22.0    12248\n20.0    10012\n18.0     9155\n17.0     4761\n23.0     2908\n14.0     2819\n1.0      2407\n15.0     2378\n24.0     2029\n13.0     1941\n12.0     1781\n9.0      1328\n11.0      834\n10.0      342\n8.0       318\n6.0       262\n7.0       180\n5.0        98\n4.0        71\n2.0        38\n3.0        34\nName: SCHL, dtype: int64\n\n\n\ndf5['WAOB'].value_counts()\n\n1    102765\n3     15399\n4      7149\n5      2017\n6      1335\n2       541\n7       355\n8       103\nName: WAOB, dtype: int64\n\n\n\ndf5['NAICSP'].value_counts(normalize=True)\n\n23       0.078464\n6111     0.070112\n722Z     0.060649\n622M     0.035793\n611M1    0.027216\n           ...   \n3133     0.000039\n31M      0.000031\n2122     0.000031\n3122     0.000015\n3131     0.000008\nName: NAICSP, Length: 269, dtype: float64"
  },
  {
    "objectID": "posts/census data/adta5240_linear_regression.html#density-plot",
    "href": "posts/census data/adta5240_linear_regression.html#density-plot",
    "title": "\n 5240 Group 1 - March the 26th, 2023\n\n",
    "section": "Density plot",
    "text": "Density plot\n\n#df5.__delitem__('ST')\n\ndf5.plot(kind='density', subplots=True, layout=(9,2), sharex=False, legend=True, fontsize=1, figsize=(12,8)) \nplt.show()"
  },
  {
    "objectID": "posts/census data/adta5240_linear_regression.html#linear-regresion",
    "href": "posts/census data/adta5240_linear_regression.html#linear-regresion",
    "title": "\n 5240 Group 1 - March the 26th, 2023\n\n",
    "section": "LINEAR REGRESION",
    "text": "LINEAR REGRESION\n\nSTEP 1: Separate the Dataset into Input & Output NumPy Arrays\n\nStore the dataframe d2 values into a NumPy array\nSeparate the array into input and output components by slicing\n\n\n#Separate the dependent from the independent variables. \narray = dflr.values\nX = array [:,[0,1,2,3,4,5,7,8,9]]\nY = array [:,6]\n\n\nX.shape\n\n(129664, 9)\n\n\n\n\nSTEP 2: Split into Input/Output Array into Training/Testing Datasets\n\nSplit the dataset into training at 67% and test at 33% with the seed = 7\n\n\n#Selection of records to include in which sub-dataset must be done randomly\n# use the for seed randomization\ntest_size = 0.33\nseed = 7\n# Split the dataset (both input & output) into training/testing datasets\nX_train, X_test, Y_train, Y_test= train_test_split(X,Y, test_size=0.33,\nrandom_state=seed)\nX_train.shape\n\n(86874, 9)"
  },
  {
    "objectID": "posts/census data/adta5240_linear_regression.html#train-model",
    "href": "posts/census data/adta5240_linear_regression.html#train-model",
    "title": "\n 5240 Group 1 - March the 26th, 2023\n\n",
    "section": " TRAIN MODEL ",
    "text": "TRAIN MODEL \n\nSTEP 3: Build and Train the Model\n\nAssign LinearRegression to the model\nTrain the model\nPrint the intercept and coefficients\nPrint the list of the coefficients with their correspondent variable name\n\n\n#Assign LinearRegression to the model\nmodel = LinearRegression()\n\n\n#Train the model\nreg = model.fit(X_train, Y_train)\n\n\n#Print the intercept and coefficients\nprint (\"Intercept:\", reg.intercept_)\nprint (\"Coefficients:\", reg.coef_)\n\nIntercept: -93421.23353624818\nCoefficients: [  1302.61387251 -14080.96350139  19626.06457212  48782.53278423\n    597.31323087  11801.78875007  63193.35845697  15483.08043019\n  46672.8424576 ]\n\n\n\n#Print the list of the coefficients with their correspondent variable name\nnames_2 = ['WKHP','BACHE','AEU','GRADU','AGEP','US','OWNERW','MALE','SALARYW']\ncoeffs_zip = zip(names_2, reg.coef_)\n# Convert iterator into a set\ncoeffs = set(coeffs_zip)\n# Print (coeffs)\nfor coef in coeffs:\n    print (coef, \"\\n\")\n\n('BACHE', -14080.963501386306) \n\n('US', 11801.788750073873) \n\n('SALARYW', 46672.842457602696) \n\n('GRADU', 48782.53278422593) \n\n('WKHP', 1302.6138725146875) \n\n('AGEP', 597.3132308680757) \n\n('OWNERW', 63193.35845697323) \n\n('AEU', 19626.064572124906) \n\n('MALE', 15483.08043018768)"
  },
  {
    "objectID": "posts/census data/adta5240_linear_regression.html#score-model",
    "href": "posts/census data/adta5240_linear_regression.html#score-model",
    "title": "\n 5240 Group 1 - March the 26th, 2023\n\n",
    "section": " SCORE MODEL ",
    "text": "SCORE MODEL"
  },
  {
    "objectID": "posts/census data/adta5240_linear_regression.html#step-4-calculate-r-squared",
    "href": "posts/census data/adta5240_linear_regression.html#step-4-calculate-r-squared",
    "title": "\n 5240 Group 1 - March the 26th, 2023\n\n",
    "section": "STEP 4: Calculate R-Squared",
    "text": "STEP 4: Calculate R-Squared\n\nCalculate the R-Squared\nPrint the score\n\n** Note: The higher the R-squared, the better (0 – 100%). Depending on the model, the best models score above 83%. The R-squared value tells us how well the independent variables predict the dependent variable, which is very low. Think about how you could increase the R-squared. What variables would you use?\n\n#What is this for???\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n\nLinearRegression(n_jobs=1, normalize=False)\n\n\n\n#Calculate the R-Squared\nR_squared = model.score(X_test, Y_test)\nprint(\"R-squared: \", R_squared)\n\nR-squared:  0.23681191222626774\n\n\n\nStep 5: Prediction\n\nExecute model prediction\nWe have now trained the model. Let’s use the trained model to predict the “Salary of an individual” WAGP\nWe are using the following predictors:\nWKHP: Works 50 hours a week\nBACHE: has also 0\nAEU: then will be = 0\nGRADU: Let say has and MBA = 1\nAGEP: He is a Male 30 years old\nUS: US citizent = 1\nOWNER: Then will be =0\nMALE: Will be = 1\nSALARYW: Work for a company =1\n\n** Note: The model predicts that the median value of owner-occupied homes in 1000 dollars in the above suburb should be around $24,144.\n\nmodel.predict([[50,0,0,1,30,1,0,1,1]])\n\narray([112369.10143762])\n\n\n\n\nCase 2 prediction:\n\nMale from the US, 30 years old with a graduate degree. Works 50 hr/wk but owner of a business\n\n\nmodel.predict([[50,0,0,1,30,1,1,1,0]])\n\narray([160079.3794644])\n\n\n\n\nCase 3 prediction:\n\nMale from Asia or Europe, 30 years old with a graduate degree. Works 50 hr/wk but owner of a business\n\n\nmodel.predict([[50,0,0,1,30,0,1,1,0]])\n\narray([163760.67114452])\n\n\n\n\nCase 4 prediction:\n\nMale from US, 40 years old with a graduate or bachelor degree. Works 50 hr/wk and is a salary employee.\n\n\nmodel.predict([[50,0,0,0,40,1,0,1,1]])\n\narray([69559.70096207])\n\n\n\n\nCase 5 prediction:\n\nMale from US, 40 years old with a graduate or bachelor degree. Works 50 hr/wk and is a salary employee.\n\n\nmodel.predict([[40,0,0,1,40,0,1,0,1]])"
  },
  {
    "objectID": "posts/census data/adta5240_linear_regression.html#evaluate-models",
    "href": "posts/census data/adta5240_linear_regression.html#evaluate-models",
    "title": "\n 5240 Group 1 - March the 26th, 2023\n\n",
    "section": " EVALUATE MODELS ",
    "text": "EVALUATE MODELS \n\nStep 6: Train & Score Model 2 Using K-Fold Cross Validation Data Split\n\nSpecify the k-size to 10\nFix the random seed to 7\nSplit the entire data set\nObtain the Mean squared error\nTrain the model and run K-fold cross-validation\nPrint results\n\n\n# Evaluate the algorithm\n# Specify the K-size\nnum_folds = 10\n# Fix the random seed\n# must use the same seed value so that the same subsets can be obtained\n# for each time the process is repeated\nseed = 7\n# Split the whole data set into folds\nkfold= KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n# For Linear regression, we can use MSE (mean squared error) value\n# to evaluate the model/algorithm\nscoring = 'neg_mean_squared_error'\n\n\n# Train the model and run K-foLd cross-validation to validate/evaluate the model\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n# Print out the evaluation results\n# Result: the average of all the results obtained from the k-fold crossvalidation\nprint(\"Average of all results from the K-fold Cross-Validation\",end=\"\")\nprint(\",using negative mean squared error:\",results.mean())\n\nAverage of all results from the K-fold Cross-Validation,using negative mean squared error: -3525537755.348807\n\n\n\n\nStep 7: Score Using Explained Variance\nLet’s use a different scoring parameter. Here we use the Explained Variance. The best possible score is 1.0; lower values are worse. - Specify the k-size to 10 - Set the seed to 7 - Split the entire data set - Obtain the explained variance score - Train the model and run K-fold cross-validation - Print results\n\n# Evaluate the algorithm\n# Specify the K-size\nnum_folds = 10\n# Fix the random seed must use the same seed value so that the same subsets\n# can be obtained\n# for each time the process is repeated\nseed = 7\n# Split the whole data set into folds\nkfold= KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n# For Linear regression, we can use explained variance value to evaluate the model/algorithm\nscoring = 'explained_variance'\n\n\n# Train the model and run K-foLd cross-validation to validate/evaluate the model\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n# Print out the evaluation results\n# Result: the average of all the results obtained from the k-fold crossvalidation\nprint(\"Average of all results from the K-fold Cross Validation\",end=\"\")\nprint(\" using exlpained variance:\",results.mean())\n\nAverage of all results from the K-fold Cross Validation using exlpained variance: 0.23548372748977942\n\n\n\nsns.pairplot(df5,x_vars=['AGEP','WKHP'],y_vars=['WAGP'],hue='BACHE',height=4.1,aspect=2.1)\n\n\n\n\n\nfig, ax = plt.subplots(figsize=[15,7])\nsns.heatmap(df5.corr(),annot=True,lw=1,ax=ax)\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\ndf5.loc[df5['WAGP'] &gt; 0   , 'WAGP'] = df5['WAGP']/100000"
  },
  {
    "objectID": "posts/Crypto/index.html",
    "href": "posts/Crypto/index.html",
    "title": "Analysis and Prediction on Cryptocurrencies",
    "section": "",
    "text": "Analyzes the trends and pricing of tradition investment options and compares them to cryptocurrencies.\nCryptocurrencies Presentation Download\nCryptocurrencies Report Download\nCryptocurrencies Dataset Download\n\n\nFor this Project, Eric Droegemeier, Tejaskumar Patel, Brad Reese and I analyzed data for traditional investment options such as stocks, gold, silver, and other commodities and compared it to several cryptocurrencies. We looked for correlations and trends between specific sectors and the price of Bitcoin. We predicted the price of Bitcoin at the end of 2023 based of data preceeding September 2022, For this project, we used Microsoft Office applications such as Excel."
  },
  {
    "objectID": "posts/MLB/index.html",
    "href": "posts/MLB/index.html",
    "title": "The Effect of the Pitch Clock on MLB Pitchers",
    "section": "",
    "text": "Analyzes the effect of the Pitch Clock on MLB Pitchers during the 2023 MLB Season.\n\n\nMLB Pitchclock Code Download\nMLB Pitchclock Code 2 Download\nMLB Pitchclock Report Download\nMLB Pitchclock Dataset Download\nFor this Project, Diana Bergeman, Eric Droegemeier, Triniti Lemmons, Mauricio Sanchez and I analyzed data on the effect of the newly-implemented pitch clock in the MLB on pitchers. We looked for correlations and trends between individual and total pitching statistics from the 2022 and 2023 MLB seasons. We determined that there was minimal to no effect on pitcher performance. We primarily used Python for this project."
  },
  {
    "objectID": "posts/MLB/2023 Data Final Project w plots.html",
    "href": "posts/MLB/2023 Data Final Project w plots.html",
    "title": "Gregory Ehlinger",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nimport seaborn as sns\nsns.set(color_codes=True)\nimport matplotlib.pyplot as plt\nimport os \nos.chdir('C:\\\\Users\\\\Mauricio Sanchez\\\\Downloads')\nos.getcwd()\n\n'C:\\\\Users\\\\Mauricio Sanchez\\\\Downloads'\n\n\n\n# Specify the location of the dataset.\nMLB = 'PitchingVolations.csv'\n\n\n# Load the data into a Pandas DataFrame\ndf= pd.read_csv (MLB, header=None)\n\n\ndf.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n\n0\nJose\nA. Ferrer\nWSN\n4.1\n63\n29\n34\n0.00\n0.31\n0.19\n...\n3.43\n0.07\n0.1\n0.3\n18.9\n0.50\n72.0\n24017\n678606\n1\n\n\n1\nFernando\nAbad\nCOL\n5.0\n116\n46\n70\n5.40\n0.08\n0.08\n...\n7.20\n-0.02\n-0.1\n0.0\n19.2\n0.41\n57.0\n4994\n472551\n0\n\n\n2\nAndrew\nAbbott\nCIN\n41.2\n674\n240\n434\n2.38\n0.29\n0.08\n...\n4.37\n1.20\n0.9\n1.8\n17.9\n0.43\n87.0\n29911\n671096\n0\n\n\n3\nCory\nAbbott\nWSN\n17.0\n294\n112\n182\n4.24\n0.18\n0.12\n...\n4.85\n-0.25\n0.0\n0.1\n18.1\n0.35\n97.0\n20277\n676265\n4\n\n\n4\nBryan\nAbreu\nHOU\n42.0\n746\n282\n464\n2.79\n0.37\n0.10\n...\n2.89\n-0.09\n0.6\n0.8\n19.9\n0.34\n139.0\n16609\n650556\n3\n\n\n\n\n5 rows × 23 columns\n\n\n\n\ncol_names = ['FirstName', 'LastName', 'Team', 'IP', 'Pitches','Balls', 'Strikes', 'ERA', 'K%', 'BB%', 'HR/9', 'FIP', 'ERA-', 'xFIP', 'WPA' , 'WAR', 'RA9-WAR', 'Pace (pi)', 'HardHit%', 'Stuff+', 'playerid', 'mlbamid', 'Timer Violations']\n\n\ndf.columns = col_names\n\n\n# Look at the first 5 rows of data\ndf.head()\n\n\n\n\n\n\n\n\nFirstName\nLastName\nTeam\nIP\nPitches\nBalls\nStrikes\nERA\nK%\nBB%\n...\nxFIP\nWPA\nWAR\nRA9-WAR\nPace (pi)\nHardHit%\nStuff+\nplayerid\nmlbamid\nTimer Violations\n\n\n\n\n0\nJose\nA. Ferrer\nWSN\n4.1\n63\n29\n34\n0.00\n0.31\n0.19\n...\n3.43\n0.07\n0.1\n0.3\n18.9\n0.50\n72.0\n24017\n678606\n1\n\n\n1\nFernando\nAbad\nCOL\n5.0\n116\n46\n70\n5.40\n0.08\n0.08\n...\n7.20\n-0.02\n-0.1\n0.0\n19.2\n0.41\n57.0\n4994\n472551\n0\n\n\n2\nAndrew\nAbbott\nCIN\n41.2\n674\n240\n434\n2.38\n0.29\n0.08\n...\n4.37\n1.20\n0.9\n1.8\n17.9\n0.43\n87.0\n29911\n671096\n0\n\n\n3\nCory\nAbbott\nWSN\n17.0\n294\n112\n182\n4.24\n0.18\n0.12\n...\n4.85\n-0.25\n0.0\n0.1\n18.1\n0.35\n97.0\n20277\n676265\n4\n\n\n4\nBryan\nAbreu\nHOU\n42.0\n746\n282\n464\n2.79\n0.37\n0.10\n...\n2.89\n-0.09\n0.6\n0.8\n19.9\n0.34\n139.0\n16609\n650556\n3\n\n\n\n\n5 rows × 23 columns\n\n\n\n\ndf.isnull().sum()\n\nFirstName            0\nLastName             0\nTeam                 0\nIP                   0\nPitches              0\nBalls                0\nStrikes              0\nERA                  0\nK%                   0\nBB%                  0\nHR/9                 0\nFIP                  0\nERA-                 0\nxFIP                 0\nWPA                  0\nWAR                  0\nRA9-WAR              0\nPace (pi)            0\nHardHit%             0\nStuff+              17\nplayerid             0\nmlbamid              0\nTimer Violations     0\ndtype: int64\n\n\n\nprint(df.shape)\n\n(751, 23)\n\n\n\nprint(df.dtypes)\n\nFirstName            object\nLastName             object\nTeam                 object\nIP                  float64\nPitches               int64\nBalls                 int64\nStrikes               int64\nERA                 float64\nK%                  float64\nBB%                 float64\nHR/9                float64\nFIP                 float64\nERA-                  int64\nxFIP                float64\nWPA                 float64\nWAR                 float64\nRA9-WAR             float64\nPace (pi)           float64\nHardHit%            float64\nStuff+              float64\nplayerid              int64\nmlbamid               int64\nTimer Violations      int64\ndtype: object\n\n\n\nprint(df.describe())\n\n               IP      Pitches       Balls      Strikes         ERA  \\\ncount  751.000000   751.000000  751.000000   751.000000  751.000000   \nmean    31.479893   528.605859  191.234354   337.371505    5.997909   \nstd     29.393286   478.460422  171.259673   308.717164    9.177736   \nmin      0.100000     4.000000    0.000000     3.000000    0.000000   \n25%      7.150000   137.500000   52.000000    83.500000    3.035000   \n50%     25.000000   423.000000  154.000000   273.000000    4.310000   \n75%     42.100000   697.000000  253.000000   445.500000    6.170000   \nmax    118.100000  1906.000000  733.000000  1255.000000  108.000000   \n\n               K%         BB%        HR/9         FIP         ERA-  \\\ncount  751.000000  751.000000  751.000000  751.000000   751.000000   \nmean     0.207044    0.095792    1.484927    5.242836   140.025300   \nstd      0.092906    0.063023    2.389082    4.389534   216.792443   \nmin      0.000000    0.000000    0.000000   -0.710000     0.000000   \n25%      0.160000    0.060000    0.520000    3.410000    70.000000   \n50%      0.210000    0.090000    1.060000    4.360000   101.000000   \n75%      0.260000    0.120000    1.660000    5.515000   141.000000   \nmax      0.670000    0.630000   27.000000   60.290000  2593.000000   \n\n             xFIP         WPA         WAR     RA9-WAR   Pace (pi)    HardHit%  \\\ncount  751.000000  751.000000  751.000000  751.000000  751.000000  751.000000   \nmean     4.974434   -0.034394    0.316112    0.315712   18.245273    0.406884   \nstd      2.685985    0.725944    0.659712    0.830096    2.073123    0.122728   \nmin      0.290000   -2.720000   -0.900000   -1.900000    8.500000    0.000000   \n25%      3.825000   -0.370000   -0.100000   -0.100000   17.500000    0.350000   \n50%      4.540000   -0.020000    0.100000    0.100000   18.500000    0.400000   \n75%      5.355000    0.175000    0.500000    0.600000   19.500000    0.460000   \nmax     35.080000    3.940000    4.000000    4.000000   23.800000    1.000000   \n\n           Stuff+      playerid        mlbamid  Timer Violations  \ncount  734.000000    751.000000     751.000000        751.000000  \nmean    96.694823  17989.014647  630512.246338          0.699068  \nstd     25.373705   5778.541275   53869.292443          1.168469  \nmin   -213.000000   1157.000000  425794.000000          0.000000  \n25%     90.000000  14462.500000  605473.000000          0.000000  \n50%     99.000000  18454.000000  656266.000000          0.000000  \n75%    108.000000  21508.500000  669003.000000          1.000000  \nmax    161.000000  31839.000000  701643.000000         11.000000  \n\n\n\ndf.hist(edgecolor= 'black',figsize=(23,12))\nplt.show()\n\n\n\n\n\ndf.plot(kind='density', subplots=True, layout= (5,4), sharex=False,legend=True, fontsize=1, figsize= (25,25))\nplt.show()\n\n\n\n\n\ndf.plot(kind=\"box\", subplots=True, layout=(5,4), sharex=False, figsize=(25,50))\nplt.show()\n\n\n\n\n\npd.options.display.float_format = '{:,.3f}'.format\n\n\ndf.corr()\n\nC:\\Users\\LordG\\AppData\\Local\\Temp\\ipykernel_22804\\1134722465.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  df.corr()\n\n\n\n\n\n\n\n\n\nIP\nPitches\nBalls\nStrikes\nERA\nK%\nBB%\nHR/9\nFIP\nERA-\nxFIP\nWPA\nWAR\nRA9-WAR\nPace (pi)\nHardHit%\nStuff+\nplayerid\nmlbamid\nTimer Violations\n\n\n\n\nIP\n1.000\n0.995\n0.982\n0.996\n-0.200\n0.256\n-0.181\n-0.139\n-0.232\n-0.198\n-0.272\n0.117\n0.730\n0.640\n0.148\n-0.115\n0.139\n-0.098\n-0.146\n0.204\n\n\nPitches\n0.995\n1.000\n0.994\n0.998\n-0.189\n0.261\n-0.153\n-0.132\n-0.222\n-0.187\n-0.262\n0.068\n0.701\n0.594\n0.161\n-0.108\n0.144\n-0.092\n-0.143\n0.215\n\n\nBalls\n0.982\n0.994\n1.000\n0.986\n-0.183\n0.253\n-0.119\n-0.130\n-0.212\n-0.182\n-0.248\n0.043\n0.666\n0.565\n0.168\n-0.105\n0.142\n-0.092\n-0.143\n0.225\n\n\nStrikes\n0.996\n0.998\n0.986\n1.000\n-0.191\n0.264\n-0.172\n-0.132\n-0.227\n-0.189\n-0.269\n0.081\n0.717\n0.607\n0.156\n-0.109\n0.145\n-0.092\n-0.143\n0.208\n\n\nERA\n-0.200\n-0.189\n-0.183\n-0.191\n1.000\n-0.302\n0.218\n0.707\n0.829\n0.999\n0.629\n-0.127\n-0.182\n-0.228\n-0.093\n0.277\n-0.049\n0.025\n0.006\n-0.073\n\n\nK%\n0.256\n0.261\n0.253\n0.264\n-0.302\n1.000\n-0.057\n-0.224\n-0.428\n-0.298\n-0.576\n0.225\n0.354\n0.287\n0.420\n-0.127\n0.435\n0.029\n0.021\n0.069\n\n\nBB%\n-0.181\n-0.153\n-0.119\n-0.172\n0.218\n-0.057\n1.000\n-0.003\n0.328\n0.214\n0.563\n-0.085\n-0.208\n-0.175\n0.153\n-0.003\n0.055\n0.048\n0.039\n0.017\n\n\nHR/9\n-0.139\n-0.132\n-0.130\n-0.132\n0.707\n-0.224\n-0.003\n1.000\n0.872\n0.709\n0.348\n-0.117\n-0.206\n-0.168\n-0.166\n0.282\n-0.080\n-0.016\n-0.036\n-0.062\n\n\nFIP\n-0.232\n-0.222\n-0.212\n-0.227\n0.829\n-0.428\n0.328\n0.872\n1.000\n0.828\n0.745\n-0.147\n-0.287\n-0.237\n-0.180\n0.287\n-0.113\n-0.002\n-0.019\n-0.073\n\n\nERA-\n-0.198\n-0.187\n-0.182\n-0.189\n0.999\n-0.298\n0.214\n0.709\n0.828\n1.000\n0.624\n-0.125\n-0.180\n-0.226\n-0.094\n0.271\n-0.048\n0.024\n0.006\n-0.071\n\n\nxFIP\n-0.272\n-0.262\n-0.248\n-0.269\n0.629\n-0.576\n0.563\n0.348\n0.745\n0.624\n1.000\n-0.134\n-0.285\n-0.240\n-0.201\n0.177\n-0.175\n-0.006\n-0.012\n-0.060\n\n\nWPA\n0.117\n0.068\n0.043\n0.081\n-0.127\n0.225\n-0.085\n-0.117\n-0.147\n-0.125\n-0.134\n1.000\n0.482\n0.694\n0.002\n-0.111\n0.089\n0.007\n0.019\n0.021\n\n\nWAR\n0.730\n0.701\n0.666\n0.717\n-0.182\n0.354\n-0.208\n-0.206\n-0.287\n-0.180\n-0.285\n0.482\n1.000\n0.822\n0.092\n-0.129\n0.162\n-0.049\n-0.078\n0.147\n\n\nRA9-WAR\n0.640\n0.594\n0.565\n0.607\n-0.228\n0.287\n-0.175\n-0.168\n-0.237\n-0.226\n-0.240\n0.694\n0.822\n1.000\n0.057\n-0.150\n0.130\n-0.043\n-0.063\n0.127\n\n\nPace (pi)\n0.148\n0.161\n0.168\n0.156\n-0.093\n0.420\n0.153\n-0.166\n-0.180\n-0.094\n-0.201\n0.002\n0.092\n0.057\n1.000\n-0.113\n0.550\n-0.008\n-0.027\n0.028\n\n\nHardHit%\n-0.115\n-0.108\n-0.105\n-0.109\n0.277\n-0.127\n-0.003\n0.282\n0.287\n0.271\n0.177\n-0.111\n-0.129\n-0.150\n-0.113\n1.000\n-0.060\n0.097\n0.075\n-0.070\n\n\nStuff+\n0.139\n0.144\n0.142\n0.145\n-0.049\n0.435\n0.055\n-0.080\n-0.113\n-0.048\n-0.175\n0.089\n0.162\n0.130\n0.550\n-0.060\n1.000\n0.061\n0.032\n0.047\n\n\nplayerid\n-0.098\n-0.092\n-0.092\n-0.092\n0.025\n0.029\n0.048\n-0.016\n-0.002\n0.024\n-0.006\n0.007\n-0.049\n-0.043\n-0.008\n0.097\n0.061\n1.000\n0.881\n-0.117\n\n\nmlbamid\n-0.146\n-0.143\n-0.143\n-0.143\n0.006\n0.021\n0.039\n-0.036\n-0.019\n0.006\n-0.012\n0.019\n-0.078\n-0.063\n-0.027\n0.075\n0.032\n0.881\n1.000\n-0.115\n\n\nTimer Violations\n0.204\n0.215\n0.225\n0.208\n-0.073\n0.069\n0.017\n-0.062\n-0.073\n-0.071\n-0.060\n0.021\n0.147\n0.127\n0.028\n-0.070\n0.047\n-0.117\n-0.115\n1.000\n\n\n\n\n\n\n\n\nplt.figure(figsize =(16,10))\nsns.heatmap(df.corr(), annot=True)\nplt.show()\n\nC:\\Users\\LordG\\AppData\\Local\\Temp\\ipykernel_22804\\1236886484.py:2: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  sns.heatmap(df.corr(), annot=True)\n\n\n\n\n\n\ndf2= df[['ERA','K%', 'BB%', 'HardHit%','Timer Violations']]\n\n\ndf2.corr()\n\n\n\n\n\n\n\n\nERA\nK%\nBB%\nHardHit%\nTimer Violations\n\n\n\n\nERA\n1.000\n-0.302\n0.218\n0.277\n-0.073\n\n\nK%\n-0.302\n1.000\n-0.057\n-0.127\n0.069\n\n\nBB%\n0.218\n-0.057\n1.000\n-0.003\n0.017\n\n\nHardHit%\n0.277\n-0.127\n-0.003\n1.000\n-0.070\n\n\nTimer Violations\n-0.073\n0.069\n0.017\n-0.070\n1.000\n\n\n\n\n\n\n\n\nsns.pairplot(df2, height=2);\nplt.show()\n\n\n\n\n\nplt.figure(figsize =(20,12))\nsns.heatmap(df2.corr(), annot=True)\nplt.show()\n\n\n\n\n\nplt.figure(figsize =(20,12))\nsns.heatmap(df2.corr(), cmap=\"Blues\", annot=True, annot_kws={\"fontsize\":20})\nplt.show()\n\n\n\n\n\ndf2 = df[['Pace (pi)','K%']]\n\n\ndf3 = df[['ERA','xFIP']]\n\n\ndf2.head()\n\n\n\n\n\n\n\n\nPace (pi)\nK%\n\n\n\n\n0\n18.9\n0.31\n\n\n1\n19.2\n0.08\n\n\n2\n17.9\n0.29\n\n\n3\n18.1\n0.18\n\n\n4\n19.9\n0.37\n\n\n\n\n\n\n\n\ndf3.head()\n\n\n\n\n\n\n\n\nERA\nxFIP\n\n\n\n\n0\n0.00\n3.43\n\n\n1\n5.40\n7.20\n\n\n2\n2.38\n4.37\n\n\n3\n4.24\n4.85\n\n\n4\n2.79\n2.89\n\n\n\n\n\n\n\n\ndf4 = df[['Pitches','Strikes']]\n\n\ndf4.head()\n\n\n\n\n\n\n\n\nPitches\nStrikes\n\n\n\n\n0\n63\n34\n\n\n1\n116\n70\n\n\n2\n674\n434\n\n\n3\n294\n182\n\n\n4\n746\n464\n\n\n\n\n\n\n\n\narray = df2.values\nx = array[:,0]\ny = array[:,1]\nplt.plot(x,y,'.')\nplt.xlabel(\"Pace (pi)\")\nplt.ylabel(\"K%\")\n\nText(0, 0.5, 'K%')\n\n\n\n\n\n\narray = df3.values\nx = array[:,0]\ny = array[:,1]\nplt.plot(x,y,'.')\nplt.xlabel(\"ERA\")\nplt.ylabel(\"xFIP\")\nplt.title('Hello')\n\nText(0.5, 1.0, 'Hello')\n\n\n\n\n\n\narray = df4.values\nx = array[:,0]\ny = array[:,1]\nplt.plot(x,y,'.')\nplt.xlabel(\"Pitches\")\nplt.ylabel(\"Strikes\")\nplt.grid(False)"
  },
  {
    "objectID": "posts/nonprofit/index.html",
    "href": "posts/nonprofit/index.html",
    "title": "Analysis and Prediction on Donations",
    "section": "",
    "text": "Analyzes the history on donations to a non-profit organization and compare the performance of prediction models for future donations.\nDonation Project Download\nDonation Report Download\nNon-profit Donation Dataset Download\n\n\nFor this Project, Eric Droegemeier, Jenetty Immaraj, and I analyzed data on historical donation data. We then created several predicted models and compared which model performed the best.We then used the best performing model to predict which former donors to target to maximize donor dollars for the next donation marketing campaign. For this project, we used SAS Enterprise Miner."
  },
  {
    "objectID": "blogs/First Blog/index.html",
    "href": "blogs/First Blog/index.html",
    "title": "Intro to EDA in RStudio",
    "section": "",
    "text": "Here I will describe the basics of performing EDA in RStudio.\nTo began Exploratory Data Analysis (EDA) in RStudio, we first must load in our dataset. RStudio comes equipped with several datasets; so to describe the EDA process in RStudio, I have decided to use the “mtcars” dataset. The “mtcars” dataset pulls information from the 1970 Motor Trend US magazine about various characteristics from car models of the time. Some packages that are helpful with the EDA process include “dplyr”, “ggplot2”, and “tidyr”.\n\n# Deleting the \"#\" symbol and running the code will install the package automatically.\n\n#install.packages(\"dplyr\")\n#install.packages(\"ggplot2\")\n#install.packages(\"tidyr\")\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\nlibrary(tidyr)\n\nWarning: package 'tidyr' was built under R version 4.3.2\n\ndata(\"mtcars\")\n\nTo install the mentioned packages and load the “mtcars” dataset follow the code above; however,\nNow to start exploring, it is good to look at the structure and the variables of the dataset\n\n# Will print out the column names, our variables, of the dataset\ncolnames(mtcars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n# Will print out the structure of the data\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n# Will print out the summary statistics of the variables of the dataset\nsummary(mtcars)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n\nTo get a better visualization of the distribution of the data, we can create several different plots. For this application, box plots are great! They visualize the summary statistics listed in the code block above.\n\n# Creates box plots for each variable in our dataset\npar(mfrow = c(2, 6), mar = c(1, 1, 2, 1))\n\nfor (i in 1:ncol(mtcars)) {\n  boxplot(mtcars[, i], main = names(mtcars)[i], col = \"green\", border = \"black\") \n}\n\n\n\n\nNow that we know the name of our variables, an overview of the dataset, and distribution of the dataset, we can explore the relationship among the variables.\nTo find correlations/relationships, we must make sure that our dataset does not have any missing values.\n\n# Will print the number of missing values in the dataset\nsum(is.na(mtcars))\n\n[1] 0\n\n\nIn this dataset, we do not have any missing values which means we will be able to proceed without any additional steps. However, if the dataset was missing values, then data cleaning would be necessary. To handle missing values, excluding or imputing records with missing values would be necessary. Additionally, we need to make sure all the variables are numerical.\n\n# Will print out the structure of the data\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\nLooking at the structure of the data shows us that all variables are already “num”.\nTo create a correlation matrix, installing the “corrplot” package would be beneficial.\n\n# Visualize the correlation matrix\n#install.packages(\"corrplot\") &lt;- remove the \"#\" symbol to have RStudio automatically install the \"corrplot\" package\n\nlibrary(corrplot)\n\nWarning: package 'corrplot' was built under R version 4.3.2\n\n\ncorrplot 0.92 loaded\n\ncorrelation_matrix &lt;- cor(mtcars)\n\n# Creates the correlation matrix with numerical values for the variables\n\ncorrelation_matrix &lt;- cor(mtcars)\n\n# Prints our correlation matrix\nprint(correlation_matrix)\n\n            mpg        cyl       disp         hp        drat         wt\nmpg   1.0000000 -0.8521620 -0.8475514 -0.7761684  0.68117191 -0.8676594\ncyl  -0.8521620  1.0000000  0.9020329  0.8324475 -0.69993811  0.7824958\ndisp -0.8475514  0.9020329  1.0000000  0.7909486 -0.71021393  0.8879799\nhp   -0.7761684  0.8324475  0.7909486  1.0000000 -0.44875912  0.6587479\ndrat  0.6811719 -0.6999381 -0.7102139 -0.4487591  1.00000000 -0.7124406\nwt   -0.8676594  0.7824958  0.8879799  0.6587479 -0.71244065  1.0000000\nqsec  0.4186840 -0.5912421 -0.4336979 -0.7082234  0.09120476 -0.1747159\nvs    0.6640389 -0.8108118 -0.7104159 -0.7230967  0.44027846 -0.5549157\nam    0.5998324 -0.5226070 -0.5912270 -0.2432043  0.71271113 -0.6924953\ngear  0.4802848 -0.4926866 -0.5555692 -0.1257043  0.69961013 -0.5832870\ncarb -0.5509251  0.5269883  0.3949769  0.7498125 -0.09078980  0.4276059\n            qsec         vs          am       gear        carb\nmpg   0.41868403  0.6640389  0.59983243  0.4802848 -0.55092507\ncyl  -0.59124207 -0.8108118 -0.52260705 -0.4926866  0.52698829\ndisp -0.43369788 -0.7104159 -0.59122704 -0.5555692  0.39497686\nhp   -0.70822339 -0.7230967 -0.24320426 -0.1257043  0.74981247\ndrat  0.09120476  0.4402785  0.71271113  0.6996101 -0.09078980\nwt   -0.17471588 -0.5549157 -0.69249526 -0.5832870  0.42760594\nqsec  1.00000000  0.7445354 -0.22986086 -0.2126822 -0.65624923\nvs    0.74453544  1.0000000  0.16834512  0.2060233 -0.56960714\nam   -0.22986086  0.1683451  1.00000000  0.7940588  0.05753435\ngear -0.21268223  0.2060233  0.79405876  1.0000000  0.27407284\ncarb -0.65624923 -0.5696071  0.05753435  0.2740728  1.00000000\n\n\nAn alternative way to look at the correlation matrix is by adding color.\n\n# Creates the correlation matrix but changes the numerical value of the correlation with a color\n# Red indicates a negative correlation\n# Blue indicates a positive correlation\n\ncorrplot(correlation_matrix, method = \"color\")\n\n\n\n\nTo expand upon the correlations between variables, if we wanted to focus on a single variable, like a target variable. For this example, I will be using “mpg” as the target variable.\n\n# Find the variables with the highest absolute correlation with mpg\nmpg_correlation &lt;- correlation_matrix[\"mpg\", ]\nabs_correlation &lt;- abs(mpg_correlation)\n\n# Sort in descending order\nsorted_correlation &lt;- sort(abs_correlation, decreasing = TRUE)\n\n# Print the variables and their absolute correlation with mpg\nprint(sorted_correlation)\n\n      mpg        wt       cyl      disp        hp      drat        vs        am \n1.0000000 0.8676594 0.8521620 0.8475514 0.7761684 0.6811719 0.6640389 0.5998324 \n     carb      gear      qsec \n0.5509251 0.4802848 0.4186840 \n\n\nAs we can see, weight has the highest absolute correlation to mpg.\n\n# Create a scatterplot between weight and mpg\nplot(mtcars$wt, mtcars$mpg, \n     main = \"Scatterplot of Weight vs. MPG\",\n     xlab = \"Weight\",\n     ylab = \"Miles per Gallon\",\n     col = \"green\",\n     pch= 19) #stylizes the data points on the graph\n\n# Adds the regression line which would in\nabline(lm(mpg ~ wt, data = mtcars), col = \"black\", lwd = 2)\n\n\n\n\nTo find the equation for the black regression line in the scatterplot, use the code below.\n\n# Fit a linear regression model\nlm_model &lt;- lm(mpg ~ wt, data = mtcars)\n\n# Print the regression formula\nprint(summary(lm_model))\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nBased on the correlation taken from the matrix of -0.8676594 and this graph, we can see that there is a strong negative relationship between the two variables weight and mpg.\nAdditional EDA can be performed on this dataset but this should serve as a basic introduction on how to conduct EDA in RStudio."
  },
  {
    "objectID": "blogs/Second Blog/index.html",
    "href": "blogs/Second Blog/index.html",
    "title": "Intro to Linear Regression in RStudio",
    "section": "",
    "text": "Here I will describe the basics of creating a linear regression model in RStudio.\nTo create a linear regression model in RStudio, we first must load in our dataset. RStudio comes equipped with several datasets; so to describe the EDA process in RStudio, I have decided to use the “mtcars” dataset. The “mtcars” dataset pulls information from the 1970 Motor Trend US magazine about various characteristics from car models of the time. Some packages that are beneficial to use for the creation of linear regression models include: “knitr”, “tidyverse”, “dplyr”, “ggplot2”, “rpart”, “rsample”, “caret”, and “mgcv”.\n\n# Deleting the \"#\" symbol and running the code will install the package automatically.\n\n#install.packages(\"knitr\")\n#install.packages(\"tidyverse\")\n#install.packages(\"dplyr\")\n#install.packages(\"ggplot2\")\n#install.packages(\"rpart\")\n#install.packages(\"rsample\")\n#install.packages(\"caret\")\n#install.packages(\"mgcv\")\n\nlibrary(knitr)\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\nWarning: package 'tidyr' was built under R version 4.3.2\n\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(rpart)\nlibrary(rsample)\n\nWarning: package 'rsample' was built under R version 4.3.2\n\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(mgcv)\n\nWarning: package 'mgcv' was built under R version 4.3.2\n\n\nLoading required package: nlme\n\nAttaching package: 'nlme'\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\nThis is mgcv 1.9-0. For overview type 'help(\"mgcv-package\")'.\n\n# Loads \"mtcars\" dataset.\ndata(\"mtcars\")\n\nA cleaned dataset is necessary to create a linear regression model. First we need to see if we have any missing values in the dataset\n\n# Structure fo the dataset\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n# Will print the number of missing values in the dataset\nsum(is.na(mtcars))\n\n[1] 0\n\n\nIn this dataset, we do not have any missing values which means we will be able to proceed without any additional steps. However, if the dataset was missing values, then the missing values would need to be excluded or imputed.\nBuilding on the EDA we performed on this dataset in the first blog post, “Intro to EDA in RStudio”, we will be using mpg as our target variable in our linear regression model. Before creating the model, we will need to split our dataset into training and testing sets.\n\n# Load the caret package\nlibrary(caret)\n\n# Set a seed, important in to reproduce the same results\nset.seed(123)\n\n# Split the data\nsplit_data &lt;- initial_split(mtcars, prop = 0.7)\n\n# Create the training and testing datasets\n\ntrain_data &lt;- training(split_data)\ntest_data &lt;- testing(split_data)\n\nAfter splitting the data, we can being now make estimates based on the testing data.\n\n# Creates the actual linear model\n\nlinearmodel &lt;- lm(mpg ~ ., data = train_data)\n\n# Makes predictions based on the training data.\n\npredicted_mpg &lt;- predict(linearmodel, newdata = train_data)\n\nIn order to calculate the performance of a model, we use a performance metric. One of the most commonly used metrics is MSPE, Mean Squared Prediction Error, which is the average squared difference between the predicted values and the actual values. Ideally we want a lower MSPE which would indicate a better predictive model.\n\n# Calculate the Mean Squared Prediction Error (MSPE)\n\nMSPE_Training &lt;- mean((train_data$mpg - predicted_mpg)^2)\n\nprint(MSPE_Training)\n\n[1] 5.000019\n\n\nKnowing our MSPE for the training set, we should know compare with the MSPE of our testing set. So we can use the same approach as before.\n\n# Calculate the Mean Squared Prediction Error (MSPE)\n\n# Predict the target variable on the testing set\n\npredicted_mpg_test &lt;- predict(linearmodel, newdata = test_data)\n\nMSPE_Testing &lt;- mean((test_data$mpg - predicted_mpg_test)^2)\n\nprint(MSPE_Testing)\n\n[1] 5.205016\n\n\nSo, the MSPE on the testing set provides an evaluation of how well the linear regression model generalizes and predicts on new data. A lower test MSPE would indicate a better predictive performance on the testing set, but in our case we have a higher test MSPE which means our model could be improved and things like overfitting or multicollinearity could be present."
  },
  {
    "objectID": "blogs/First Blog/index.html#exploratory-data-analysis-in-r",
    "href": "blogs/First Blog/index.html#exploratory-data-analysis-in-r",
    "title": "First Blog",
    "section": "",
    "text": "To began Exploratory Data Analysis (EDA) in R, we first must load in our dataset. RStudio comes equipped with several datasets; so to describe the EDA process in RStudio, I have decided to use the “mtcars” dataset. The “mtcars” dataset pulls information from the 1970 Motor Trend US magazine about various characteristics from car models of the time. Some packages that are helpful with the EDA process include “dplyr”, “ggplot2”, and “tidyr”.\n\n# Deleting the \"#\" symbol and running the code will install the package automatically.\n\n#install.packages(\"dplyr\")\n#install.packages(\"ggplot2\")\n#install.packages(\"tidyr\")\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\nlibrary(tidyr)\n\nWarning: package 'tidyr' was built under R version 4.3.2\n\ndata(\"mtcars\")\n\nTo install the mentioned packages and load the “mtcars” dataset follow the code above; however,\nNow to start exploring, it is good to look at the structure and the variables of the dataset\n\n# Will print out the column names, our variables, of the dataset\ncolnames(mtcars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n# Will print out the structure of the data\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n# Will print out the summary statistics of the variables of the dataset\nsummary(mtcars)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n\nTo get a better visualization of the distribution of the data, we can create several different plots. For this application, box plots are great! They visualize the summary statistics listed in the code block above.\n\n# Creates box plots for each variable in our dataset\npar(mfrow = c(2, 6), mar = c(1, 1, 2, 1))\n\nfor (i in 1:ncol(mtcars)) {\n  boxplot(mtcars[, i], main = names(mtcars)[i], col = \"green\", border = \"black\") \n}\n\n\n\n\nNow that we know the name of our variables, an overview of the dataset, and distribution of the dataset, we can explore the relationship among the variables.\nTo find correlations/relationships, we must make sure that our dataset does not have any missing values.\n\n# Will print the number of missing values in the dataset\nsum(is.na(mtcars))\n\n[1] 0\n\n\nIn this dataset, we do not have any missing values which means we will be able to proceed without any additional steps. However, if the dataset was missing values, then data cleaning would be necessary. To handle missing values, excluding or imputing records with missing values would be necessary.\nTo create a correlation matrix, installing the “corrplot” package would be beneficial.\n\n# Visualize the correlation matrix\n#install.packages(\"corrplot\") &lt;- remove the \"#\" symbol to have RStudio automatically install the \"corrplot\" package\n\nlibrary(corrplot)\n\nWarning: package 'corrplot' was built under R version 4.3.2\n\n\ncorrplot 0.92 loaded\n\ncorrelation_matrix &lt;- cor(mtcars)\n\n# Creates the correlation matrix with numerical values for the variables\n\ncorrelation_matrix &lt;- cor(mtcars)\n\n# Prints our correlation matrix\n#print(correlation_matrix)\n\nAn alternative way to look at the correlation matrix is by adding color.\n\n# Creates the correlation matrix but changes the numerical value of the correlation with a color\n# Red indicates a negative correlation\n# Blue indicates a positive correlation\n\ncorrplot(correlation_matrix, method = \"color\")"
  },
  {
    "objectID": "blogs/First Blog/index.html#exploratory-data-analysis-in-rstudio",
    "href": "blogs/First Blog/index.html#exploratory-data-analysis-in-rstudio",
    "title": "Intro to EDA in RStudio",
    "section": "",
    "text": "Here I will describe the basics of performing EDA in RStudio.\nTo began Exploratory Data Analysis (EDA) in RStudio, we first must load in our dataset. RStudio comes equipped with several datasets; so to describe the EDA process in RStudio, I have decided to use the “mtcars” dataset. The “mtcars” dataset pulls information from the 1970 Motor Trend US magazine about various characteristics from car models of the time. Some packages that are helpful with the EDA process include “dplyr”, “ggplot2”, and “tidyr”.\n\n# Deleting the \"#\" symbol and running the code will install the package automatically.\n\n#install.packages(\"dplyr\")\n#install.packages(\"ggplot2\")\n#install.packages(\"tidyr\")\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\nlibrary(tidyr)\n\nWarning: package 'tidyr' was built under R version 4.3.2\n\ndata(\"mtcars\")\n\nTo install the mentioned packages and load the “mtcars” dataset follow the code above; however,\nNow to start exploring, it is good to look at the structure and the variables of the dataset\n\n# Will print out the column names, our variables, of the dataset\ncolnames(mtcars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n# Will print out the structure of the data\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n# Will print out the summary statistics of the variables of the dataset\nsummary(mtcars)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n\nTo get a better visualization of the distribution of the data, we can create several different plots. For this application, box plots are great! They visualize the summary statistics listed in the code block above.\n\n# Creates box plots for each variable in our dataset\npar(mfrow = c(2, 6), mar = c(1, 1, 2, 1))\n\nfor (i in 1:ncol(mtcars)) {\n  boxplot(mtcars[, i], main = names(mtcars)[i], col = \"green\", border = \"black\") \n}\n\n\n\n\nNow that we know the name of our variables, an overview of the dataset, and distribution of the dataset, we can explore the relationship among the variables.\nTo find correlations/relationships, we must make sure that our dataset does not have any missing values.\n\n# Will print the number of missing values in the dataset\nsum(is.na(mtcars))\n\n[1] 0\n\n\nIn this dataset, we do not have any missing values which means we will be able to proceed without any additional steps. However, if the dataset was missing values, then data cleaning would be necessary. To handle missing values, excluding or imputing records with missing values would be necessary. Additionally, we need to make sure all the variables are numerical.\n\n# Will print out the structure of the data\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\nLooking at the structure of the data shows us that all variables are already “num”.\nTo create a correlation matrix, installing the “corrplot” package would be beneficial.\n\n# Visualize the correlation matrix\n#install.packages(\"corrplot\") &lt;- remove the \"#\" symbol to have RStudio automatically install the \"corrplot\" package\n\nlibrary(corrplot)\n\nWarning: package 'corrplot' was built under R version 4.3.2\n\n\ncorrplot 0.92 loaded\n\ncorrelation_matrix &lt;- cor(mtcars)\n\n# Creates the correlation matrix with numerical values for the variables\n\ncorrelation_matrix &lt;- cor(mtcars)\n\n# Prints our correlation matrix\nprint(correlation_matrix)\n\n            mpg        cyl       disp         hp        drat         wt\nmpg   1.0000000 -0.8521620 -0.8475514 -0.7761684  0.68117191 -0.8676594\ncyl  -0.8521620  1.0000000  0.9020329  0.8324475 -0.69993811  0.7824958\ndisp -0.8475514  0.9020329  1.0000000  0.7909486 -0.71021393  0.8879799\nhp   -0.7761684  0.8324475  0.7909486  1.0000000 -0.44875912  0.6587479\ndrat  0.6811719 -0.6999381 -0.7102139 -0.4487591  1.00000000 -0.7124406\nwt   -0.8676594  0.7824958  0.8879799  0.6587479 -0.71244065  1.0000000\nqsec  0.4186840 -0.5912421 -0.4336979 -0.7082234  0.09120476 -0.1747159\nvs    0.6640389 -0.8108118 -0.7104159 -0.7230967  0.44027846 -0.5549157\nam    0.5998324 -0.5226070 -0.5912270 -0.2432043  0.71271113 -0.6924953\ngear  0.4802848 -0.4926866 -0.5555692 -0.1257043  0.69961013 -0.5832870\ncarb -0.5509251  0.5269883  0.3949769  0.7498125 -0.09078980  0.4276059\n            qsec         vs          am       gear        carb\nmpg   0.41868403  0.6640389  0.59983243  0.4802848 -0.55092507\ncyl  -0.59124207 -0.8108118 -0.52260705 -0.4926866  0.52698829\ndisp -0.43369788 -0.7104159 -0.59122704 -0.5555692  0.39497686\nhp   -0.70822339 -0.7230967 -0.24320426 -0.1257043  0.74981247\ndrat  0.09120476  0.4402785  0.71271113  0.6996101 -0.09078980\nwt   -0.17471588 -0.5549157 -0.69249526 -0.5832870  0.42760594\nqsec  1.00000000  0.7445354 -0.22986086 -0.2126822 -0.65624923\nvs    0.74453544  1.0000000  0.16834512  0.2060233 -0.56960714\nam   -0.22986086  0.1683451  1.00000000  0.7940588  0.05753435\ngear -0.21268223  0.2060233  0.79405876  1.0000000  0.27407284\ncarb -0.65624923 -0.5696071  0.05753435  0.2740728  1.00000000\n\n\nAn alternative way to look at the correlation matrix is by adding color.\n\n# Creates the correlation matrix but changes the numerical value of the correlation with a color\n# Red indicates a negative correlation\n# Blue indicates a positive correlation\n\ncorrplot(correlation_matrix, method = \"color\")\n\n\n\n\nTo expand upon the correlations between variables, if we wanted to focus on a single variable, like a target variable. For this example, I will be using “mpg” as the target variable.\n\n# Find the variables with the highest absolute correlation with mpg\nmpg_correlation &lt;- correlation_matrix[\"mpg\", ]\nabs_correlation &lt;- abs(mpg_correlation)\n\n# Sort in descending order\nsorted_correlation &lt;- sort(abs_correlation, decreasing = TRUE)\n\n# Print the variables and their absolute correlation with mpg\nprint(sorted_correlation)\n\n      mpg        wt       cyl      disp        hp      drat        vs        am \n1.0000000 0.8676594 0.8521620 0.8475514 0.7761684 0.6811719 0.6640389 0.5998324 \n     carb      gear      qsec \n0.5509251 0.4802848 0.4186840 \n\n\nAs we can see, weight has the highest absolute correlation to mpg.\n\n# Create a scatterplot between weight and mpg\nplot(mtcars$wt, mtcars$mpg, \n     main = \"Scatterplot of Weight vs. MPG\",\n     xlab = \"Weight\",\n     ylab = \"Miles per Gallon\",\n     col = \"green\",\n     pch= 19) #stylizes the data points on the graph\n\n# Adds the regression line which would in\nabline(lm(mpg ~ wt, data = mtcars), col = \"black\", lwd = 2)\n\n\n\n\nTo find the equation for the black regression line in the scatterplot, use the code below.\n\n# Fit a linear regression model\nlm_model &lt;- lm(mpg ~ wt, data = mtcars)\n\n# Print the regression formula\nprint(summary(lm_model))\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nBased on the correlation taken from the matrix of -0.8676594 and this graph, we can see that there is a strong negative relationship between the two variables weight and mpg.\nAdditional EDA can be performed on this dataset but this should serve as a basic introduction on how to conduct EDA in RStudio."
  },
  {
    "objectID": "blogs/Second Blog/index.html#linear-regression-in-rstudio",
    "href": "blogs/Second Blog/index.html#linear-regression-in-rstudio",
    "title": "Intro to Linear Regression in RStudio",
    "section": "",
    "text": "Here I will describe the basics of creating a linear regression model in RStudio.\nTo create a linear regression model in RStudio, we first must load in our dataset. RStudio comes equipped with several datasets; so to describe the EDA process in RStudio, I have decided to use the “mtcars” dataset. The “mtcars” dataset pulls information from the 1970 Motor Trend US magazine about various characteristics from car models of the time. Some packages that are beneficial to use for the creation of linear regression models include: “knitr”, “tidyverse”, “dplyr”, “ggplot2”, “rpart”, “rsample”, “caret”, and “mgcv”.\n\n# Deleting the \"#\" symbol and running the code will install the package automatically.\n\n#install.packages(\"knitr\")\n#install.packages(\"tidyverse\")\n#install.packages(\"dplyr\")\n#install.packages(\"ggplot2\")\n#install.packages(\"rpart\")\n#install.packages(\"rsample\")\n#install.packages(\"caret\")\n#install.packages(\"mgcv\")\n\nlibrary(knitr)\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\nWarning: package 'tidyr' was built under R version 4.3.2\n\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(rpart)\nlibrary(rsample)\n\nWarning: package 'rsample' was built under R version 4.3.2\n\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(mgcv)\n\nWarning: package 'mgcv' was built under R version 4.3.2\n\n\nLoading required package: nlme\n\nAttaching package: 'nlme'\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\nThis is mgcv 1.9-0. For overview type 'help(\"mgcv-package\")'.\n\n# Loads \"mtcars\" dataset.\ndata(\"mtcars\")\n\nA cleaned dataset is necessary to create a linear regression model. First we need to see if we have any missing values in the dataset\n\n# Structure fo the dataset\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n# Will print the number of missing values in the dataset\nsum(is.na(mtcars))\n\n[1] 0\n\n\nIn this dataset, we do not have any missing values which means we will be able to proceed without any additional steps. However, if the dataset was missing values, then the missing values would need to be excluded or imputed.\nBuilding on the EDA we performed on this dataset in the first blog post, “Intro to EDA in RStudio”, we will be using mpg as our target variable in our linear regression model. Before creating the model, we will need to split our dataset into training and testing sets.\n\n# Load the caret package\nlibrary(caret)\n\n# Set a seed, important in to reproduce the same results\nset.seed(123)\n\n# Split the data\nsplit_data &lt;- initial_split(mtcars, prop = 0.7)\n\n# Create the training and testing datasets\n\ntrain_data &lt;- training(split_data)\ntest_data &lt;- testing(split_data)\n\nAfter splitting the data, we can being now make estimates based on the testing data.\n\n# Creates the actual linear model\n\nlinearmodel &lt;- lm(mpg ~ ., data = train_data)\n\n# Makes predictions based on the training data.\n\npredicted_mpg &lt;- predict(linearmodel, newdata = train_data)\n\nIn order to calculate the performance of a model, we use a performance metric. One of the most commonly used metrics is MSPE, Mean Squared Prediction Error, which is the average squared difference between the predicted values and the actual values. Ideally we want a lower MSPE which would indicate a better predictive model.\n\n# Calculate the Mean Squared Prediction Error (MSPE)\n\nMSPE_Training &lt;- mean((train_data$mpg - predicted_mpg)^2)\n\nprint(MSPE_Training)\n\n[1] 5.000019\n\n\nKnowing our MSPE for the training set, we should know compare with the MSPE of our testing set. So we can use the same approach as before.\n\n# Calculate the Mean Squared Prediction Error (MSPE)\n\n# Predict the target variable on the testing set\n\npredicted_mpg_test &lt;- predict(linearmodel, newdata = test_data)\n\nMSPE_Testing &lt;- mean((test_data$mpg - predicted_mpg_test)^2)\n\nprint(MSPE_Testing)\n\n[1] 5.205016\n\n\nSo, the MSPE on the testing set provides an evaluation of how well the linear regression model generalizes and predicts on new data. A lower test MSPE would indicate a better predictive performance on the testing set, but in our case we have a higher test MSPE which means our model could be improved and things like overfitting or multicollinearity could be present."
  }
]